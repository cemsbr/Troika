# AUTO GENERATED BY TROIKA DISCOVERY TOOL
#
# This is a comment
# (commands are case insensitive)
# a=Application, p=CPU, h=HardDisk, l=Link, m=Memory, n=Node, c=Configuration, s=Switch, r=Scheduler, f=FileSplit Information
# Configuration must come before corresponding application
# Filesplit info must come before corresponding application
# 
# NOTE: Please make sure that filesplit locations are consistent with application size and related filesplit configs.
# ____________________________________________________________________________________________________SYNTAX_________________________________________
# [] --> represents optional args. (with default values of Hadoop v2.2)
#
# APPLICATION:  a applicationSize(size_t) applicationOwnerID(size_t) mapIntensity(double) mapSortIntensity(double) reduceIntensity(double) reduceSortIntensity(double) mapOutputPercent(double)
#               reduceOutputPercent(double) finalOutputPercent(double) clientEventID(int) rmEventID(int) queueId(size_t) [recordSize(size_t)
#               [isThereACombiner(int) combinerIntensity(double) combinerCompressionPercent(double) [mapCpuVcores(int) reduceCpuVcores(int) [mapreduceMapMemory(size_t) mapreduceReduceMemory(size_t)
#               [amResourceMB(size_t) amCpuVcores(int)]]]]]
# CONFIG:       c mapreduceJobReduces(int) [mapreduceInputFileinputformatSplitMinsize(size_t) [mapreduceInputFileinputformatSplitMaxsize(size_t)
#               [dfsBlocksize(size_t) [mapreducetaskIOSortFactor(int) [mapreduceTaskIOSortMb(size_t) [mapreduceMapSortSpillPercent(double)
#               [mapreduceJobtrackerTaskscheduler(int) [mapreduceReduceShuffleParallelcopies(int) [mapreduceReduceShuffleMergePercent(double)
#               [mapreduceReduceShuffleInputBufferPercent(double) [mapreduceReduceInputBufferPercent(double) [mapreduceJobUbertaskEnable(bool)
#               [mapreduceJobUbertaskMaxmaps(int) [mapreduceJobUbertaskMaxreduces(int) 
#               [nodemanagerResourceMemoryMB(size_t) nodemanagerResourceCpuCores(int)
#               [getMapreduceJobReduceSlowstartCompletedmaps(double) [mapred_child_java_opts(double)]]]]]]]]]]]]]]]]]
# CPU:          p numberOfCores(int) capacity(size_t) [delayType(int) [unit(int) [delayratio(double)]]]
# MEMORY:       m maxReadSpeed(size_t) maxWriteSpeed(size_t) minReadSpeed(size_t) minWriteSpeed(size_t)remainingCapacity(size_t)
#               [delayType(int) [unit(int) [delayratio(double)]]]
# HARDDISK:     h maxReadSpeed(size_t) maxWriteSpeed(size_t) minReadSpeed(size_t) minWriteSpeed(size_t) [delayType(int) [unit(int) [delayratio(double)]]]
# NODE:         n rackExpectedEventID(int) nodeType(int) expectedEventID(int) outputEventID(int)
# LINK:         l capacity(double) expectedEventID(int) masterEventID(int) workerEventID(int) [mttr(double) [mtbf(double)
#               [delayType(int) [unit(int) [delayratio(double)]]]]]
# SWITCH:       s expectedEventID(int) [masterLinkType(int)]
# SCHEDULER:    r numberOfQueues(int) capacityOfQueue1(double) capacityOfQueue2(double) ... capacityOfQueueN(double) [maxAmResourcePercent(double) [minAllocMB(size_t) maxAllocMB(size_t)
#               [minAllocVcores(int) maxAllocVcores(int) [resourceCalculator (bool)]]]]
# FILESPLIT:    f isforcedMapTaskCount(int) numberOfFilesplits(int) nodeExpectedId1(int) nodeExpectedId2(int) ... nodeExpectedIdN(int)
# ____________________________________________________________________________________________________INPUT___________________________________________

# Please replace each *value* with the corresponding value
# E.g. *finalOutputPercent* might be replaced with 99
# Remove the optional arguments that you would like to use the default values
# Make sure that the parameter order is preserved and no undefined *value* remains in the final input.txt

# SCHEDULER
r 1 100

# TOR SWITCH (RACK1)
s 2 10

# TOR SWITCH (RACK2)
s 6 11

# There is a single aggregation switch: add its event id.
# AGGREGATION SWITCH (INTER RACK)
s 5

# CONFIGURATION
c 1

# FILESPLIT LOCATIONS (in terms of expected node event type)
f 0 80 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7 1 4 9 8 7

# APPLICATION
a 10737418200 0 0.1 0.1 6.3926595278 6.3926595278 100 100 100 1 3 0

# LINK (TOR to AGGR)
l 118390264.283 10 5 2

# LINK (TOR to AGGR)
l 118390264.283 11 5 6

# LINK (to TOR)
l 123254603.776 12 2 1

# LINK (to TOR)
l 123254603.776 13 2 3

# LINK (to TOR)
l 123254603.776 14 2 4

# LINK (to TOR)
l 123254603.776 15 6 7

# LINK (to TOR)
l 123254603.776 16 6 8

# LINK (to TOR)
l 123254603.776 17 6 9

# START: NODE_localhost CONFIGURATION
p 4 105000000
# MEMORY
m 6396313600 5347737600 4299161600 3250585600 13553684480

# max read/write speed - min read/write speed
h 267356344 109216451 217356344 103216451
# NODE
n 2 1 1 12
# END: NODE_localhost CONFIGURATION

# START: NODE_node1 CONFIGURATION
p 4 105000000
# MEMORY
m 6396313600 5347737600 4299161600 3250585600 13554958336

# max read/write speed - min read/write speed
h 269701087 97202995 226909749 94266982
# NODE
n 2 2 3 13
# END: NODE_node1 CONFIGURATION

# START: NODE_node2 CONFIGURATION
p 4 105000000
# MEMORY
m 6396313600 5347737600 4299161600 3250585600 13554761728

# max read/write speed - min read/write speed
h 256397789 116388520 236397789 106388520
# NODE
n 2 3 4 14
# END: NODE_node2 CONFIGURATION

# START: NODE_node3 CONFIGURATION
p 4 105000000
# MEMORY
m 6396313600 5347737600 4299161600 3250585600 13554761728

# max read/write speed - min read/write speed
h 253044195 82522931 212509130 81579212
# NODE
n 6 3 7 15
# END: NODE_node3 CONFIGURATION

# START: NODE_node4 CONFIGURATION
p 4 105000000
# MEMORY
m 6396313600 5347737600 4299161600 3250585600 13554761728

# max read/write speed - min read/write speed
h 257555850 90149734 217495214 78549167
# NODE
n 6 3 8 16
# END: NODE_node4 CONFIGURATION

# START: NODE_node5 CONFIGURATION
p 4 105000000
# MEMORY
m 6396313600 5347737600 4299161600 3250585600 13554761728

# max read/write speed - min read/write speed
h 267382738 93610035 224354054 82498544
# NODE
n 6 3 9 17
# END: NODE_node5 CONFIGURATION